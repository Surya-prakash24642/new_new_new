
---

# ğŸ§‘â€ğŸ’» **adf-salesforce-connector**

This project provides **Azure Data Factory (ADF)** templates for automating data ingestion from Salesforce using **Salesforce Object Names** or **Salesforce Object Query Language (SOQL)**. The ingestion process is dynamically controlled through metadata stored in **Azure Blob Storage**.

## âœ¨ **Key Features**

* **Pre-built ADF Pipelines**: Ready-to-use pipelines for data ingestion from Salesforce in both Full Load and Incremental Load methods, accelerating integration and reducing development effort. ğŸš€
* **Dynamic Metadata-driven Execution**: Pipeline configurations are stored in CSV/JSON files within Blob storage, enabling flexible execution without code modifications. ğŸ”„
* **Enterprise-grade Security**: Sensitive connection credentials are securely stored and managed, adhering to best practices for data protection. ğŸ”’

## ğŸ“‹ **Prerequisites**

Before utilizing this connector, please ensure the following prerequisites are met:

1. An **Azure Blob Storage account** to store the metadata CSV/JSON files. â˜ï¸
2. An **Azure Key Vault** to store the secrets. ğŸ”‘
3. **Environment URL**: The base URL of your Salesforce organization (org), which provides access to the platform instance. ğŸŒ
4. **Client ID**: A unique identifier (typically a consumer key) associated with a connected app or API client, used for authentication and authorization within the Salesforce platform. ğŸ”
5. **Client Secret**: A secure key generated by Salesforce, which, along with the Client ID, authenticates the connected app when requesting access via OAuth 2.0. ğŸ”‘
6. **Salesforce API Version**: Specifies the API version to ensure compatibility with your Salesforce platform, allowing you to manage and track API changes effectively. âš™ï¸

## ğŸ“‚ **Source Configuration Metadata File Structure**:

The Metadata file should be a CSV or JSON file that includes the following fields:

| Field                   | Description                                                                                                                                                                                                                                                         | Example                                         |
| ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------- |
| `Id`                    | Unique identifier for each record                                                                                                                                                                                                                                   | 1, 2, 3,...,15, ...                             |
| `Source_Name`           | Name of the Data Source                                                                                                                                                                                                                                             | Salesforce FSC, Salesforce XLR8 etc.            |
| `Object_Name`           | Name of the Object                                                                                                                                                                                                                                                  | Account, Campaign, Opportunity Service etc.     |
| `Object_API_Name`       | API Name (endpoint/relative URL) of the Object in the Source                                                                                                                                                                                                        | Account, Campaign, OpportunityService etc.      |
| `Required_Columns`      | Columns required from the source. You can also use FIELDS(ALL), FIELDS(CUSTOM), or FIELDS(STANDARD), if there are no unbounded fields(fields that can contain an unlimited or very large amount of data, making them difficult to query efficiently) in the objects | Id, ParentId, FinServ\_Meeting\_Notes\_\_c etc. |
| `Target_Schema_Name`    | Target schema or Sink schema name                                                                                                                                                                                                                                   | staging, raw or any custom schema.              |
| `Target_Table_Name`     | Target table or Sink table name                                                                                                                                                                                                                                     | Account, Campaign, Opportunity\_Service etc.    |
| `Last_Modified_Column`  | Column with Last Modified datetime/timestamp in the table                                                                                                                                                                                                           | LastModifiedDate, UpdatedAt etc.                |
| `Record_Deleted_Column` | Column having the record deleted flag                                                                                                                                                                                                                               | IsDeleted, DeletedDate etc.                     |
| `Is_Active`             | Indicator if the data table/object is active for ingestion.                                                                                                                                                                                                         | 0 or 1                                          |
| `Is_Incremental`        | Indicator if the object can be loaded in incremental                                                                                                                                                                                                                | 0 or 1                                          |
| `Data_Load_Type`        | Method of data load (Full refresh or Incremental)                                                                                                                                                                                                                   | IL or FL                                        |

The metadata file is read and loaded into the DB that you specify during pipeline execution, and the data extraction process is driven by the values defined in this file.

## âš™ï¸ **Setup and Deployment**

Follow these steps to set up and deploy the ADF templates for data ingestion:

1. Open GitHub and go to the repository containing the ADF pipeline templates. ğŸ“‚
2. Navigate to the `adf` folder and identify the file name mentioned as per in the documentation. ğŸ“„
3. Click on the **Code** button and select **Download ZIP** to download the repository as a ZIP file. ğŸ“¦

## ğŸ” **Import Pipeline Template to ADF**

1. Create the **Linked Service** only (no dataset creation required), and then select the appropriate LS in the screenshot below, proceeding further by clicking 'Use this template'. ğŸ› ï¸
2. Import the pipeline template from the pipeline folder, and a new navigation menu will appear in the second screenshot.

![Screenshot 2025-04-03 123422](https://github.com/user-attachments/assets/2423656d-3774-4459-b81f-30a79e87e5f1)

*Please note that there are two different templates available as given below*:

* **PL\_Salesforce\_Ingestion**: This is a template of the pipeline that loads the data in a full refresh method with the help of Object names specified in the metadata file. Meaning, pulls the objects specified in the metadata file directly as is. ğŸ”„
* **PL\_Salesforce\_Ingestion\_Incremental**: This is the template of the pipeline that loads the data in both Full and Incremental load methods using the SOQL (Salesforce Object Query Language) according to the metadata. ğŸ“Š

![Screenshot 2025-04-03 125307](https://github.com/user-attachments/assets/93016da0-e86f-48fc-bdcb-8c1fe616143f)

3. By importing the template, the required datasets will be created automatically. ğŸ§‘â€ğŸ’»

![image](https://github.com/user-attachments/assets/f29a97cf-6502-4eea-a635-2fc33e8bb98f)

## ğŸ› ï¸ **Supported Tools**

* âœ… ADF (Azure Data Factory)

## ğŸ **Supported Destination**

* âœ… Azure SQL ğŸ—„ï¸

## Files supported

* âœ… CSV ğŸ“Š
* âœ… JSON ğŸ—‚ï¸



## ğŸš§ **Limitations**:

1. **Data Volume**: For large datasets, it may be necessary to batch or split data to ensure smooth processing, as Salesforce may struggle with handling massive data volumes all at once. ğŸ“Š
2. **Time Zone Handling**: Time zone conversions need to be handled manually, as Salesforce stores data in UTC and does not natively adjust for local time zones. ğŸŒ
3. **Changes in Data Structure**: If there are changes in Salesforce's data model (e.g., new fields or modifications to existing objects), updates to the connector or processing logic will be required to accommodate these changes. ğŸ”§

---
